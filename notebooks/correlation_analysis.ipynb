{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(news_data, stock_data, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Sentiment analysis and daily returns\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHeadline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolarity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDaily_Return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpct_change() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Clean data and correlation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\Desktop\\week1\\myvenv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\Desktop\\week1\\myvenv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\Desktop\\week1\\myvenv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Desktop\\week1\\myvenv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\Desktop\\week1\\myvenv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[64], line 49\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     46\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(news_data, stock_data, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Sentiment analysis and daily returns\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeadline\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity)\n\u001b[0;32m     50\u001b[0m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDaily_Return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpct_change() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Clean data and correlation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\Desktop\\week1\\myvenv\\Lib\\site-packages\\textblob\\blob.py:371\u001b[0m, in \u001b[0;36mBaseBlob.__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    361\u001b[0m     text,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m     clean_html\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    369\u001b[0m ):\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, basestring):\n\u001b[1;32m--> 371\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    372\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `text` argument passed to `__init__(text)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m         )\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_html:\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_html has been deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    378\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo remove HTML markup, use BeautifulSoup\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    379\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_text() function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    380\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class 'float'>"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load stock data from multiple files\n",
    "def load_stock_data(file_paths):\n",
    "    stock_dfs = []\n",
    "    for path in file_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        df['date'] = pd.to_datetime(df['date'],errors='coerce')\n",
    "        df.rename(columns={'date': 'Date', 'close': 'Close'}, inplace=True)\n",
    "        stock_dfs.append(df)\n",
    "    return pd.concat(stock_dfs, keys=[p.split('/')[-1].split('.')[0] for p in file_paths]).reset_index(level=0).rename(columns={'level_0': 'Ticker'})\n",
    "\n",
    "stock_files = [\n",
    "    'C:/Users/hp/Desktop/week1/data/AAPL_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/AMZN_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/GOOG_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/META_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/MSFT_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/NVDA_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/TSLA_historical_data.csv'\n",
    "]\n",
    "stock_data = load_stock_data(stock_file_paths)\n",
    "\n",
    "# Load and process news data\n",
    "news_data = pd.read_csv(\"C:/Users/hp/Desktop/week1/data/raw_analyst_ratings.csv\")\n",
    "news_data.columns = news_data.columns.str.strip().str.lower()\n",
    "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
    "news_data.rename(columns={'date': 'Date', 'headline': 'Headline'}, inplace=True)\n",
    "\n",
    "# Function to standardize date timezone\n",
    "def standardize_timezone(df):\n",
    "    if df['Date'].dt.tz is None:\n",
    "        df['Date'] = df['Date'].dt.tz_localize('UTC')\n",
    "    else:\n",
    "        df['Date'] = df['Date'].dt.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "# Standardize timezones\n",
    "news_data = standardize_timezone(news_data)\n",
    "stock_data = standardize_timezone(stock_data)\n",
    "\n",
    "# Merge data\n",
    "merged_data = pd.merge(news_data, stock_data, on='Date', how='outer')\n",
    "\n",
    "# Sentiment analysis and daily returns\n",
    "merged_data['Sentiment'] = merged_data['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "merged_data['Daily_Return'] = merged_data.groupby('Ticker')['Close'].pct_change() * 100\n",
    "\n",
    "# Clean data and correlation\n",
    "clean_data = merged_data.dropna(subset=['Sentiment', 'Daily_Return'])\n",
    "correlation = clean_data[['Sentiment', 'Daily_Return']].corr().iloc[0, 1]\n",
    "print(f\"Correlation: {correlation:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load stock data from multiple files\n",
    "def load_stock_data(file_paths):\n",
    "    stock_dfs = []\n",
    "    for path in file_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        df['date'] = pd.to_datetime(df['date'],errors='coerce')\n",
    "        df.rename(columns={'date': 'Date', 'close': 'Close'}, inplace=True)\n",
    "        stock_dfs.append(df)\n",
    "    return pd.concat(stock_dfs, keys=[p.split('/')[-1].split('.')[0] for p in file_paths]).reset_index(level=0).rename(columns={'level_0': 'Ticker'})\n",
    "\n",
    "# Load stock data\n",
    "stock_file_paths = [\n",
    "    'C:/Users/hp/Desktop/week1/data/AAPL_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/AMZN_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/GOOG_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/META_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/MSFT_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/NVDA_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/TSLA_historical_data.csv'\n",
    "]\n",
    "stock_data = load_stock_data(stock_file_paths)\n",
    "\n",
    "# Load and process news data\n",
    "news_data = pd.read_csv(\"C:/Users/hp/Desktop/week1/data/raw_analyst_ratings.csv\")\n",
    "news_data.columns = news_data.columns.str.strip().str.lower()\n",
    "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
    "news_data.rename(columns={'date': 'Date', 'headline': 'Headline'}, inplace=True)\n",
    "\n",
    "# Function to standardize date timezone\n",
    "def standardize_timezone(df):\n",
    "    if df['Date'].dt.tz is None:\n",
    "        df['Date'] = df['Date'].dt.tz_localize('UTC')\n",
    "    else:\n",
    "        df['Date'] = df['Date'].dt.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "# Standardize timezones\n",
    "news_data = standardize_timezone(news_data)\n",
    "stock_data = standardize_timezone(stock_data)\n",
    "\n",
    "# Merge data\n",
    "merged_data = pd.merge(news_data, stock_data, on='Date', how='outer')\n",
    "\n",
    "# Sentiment analysis and daily returns\n",
    "merged_data['Sentiment'] = merged_data['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0.0)\n",
    "merged_data['Daily_Return'] = merged_data.groupby('Ticker')['Close'].pct_change() * 100\n",
    "\n",
    "# Clean data and correlation\n",
    "clean_data = merged_data.dropna(subset=['Sentiment', 'Daily_Return'])\n",
    "correlation = clean_data[['Sentiment', 'Daily_Return']].corr().iloc[0, 1]\n",
    "print(f\"Correlation: {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between news sentiment and daily stock returns: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load stock data from multiple files\n",
    "def load_stock_data(file_paths):\n",
    "    stock_dfs = []\n",
    "    for path in file_paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df.rename(columns={'date': 'Date', 'close': 'Close'}, inplace=True)\n",
    "        stock_dfs.append(df)\n",
    "    combined_df = pd.concat(stock_dfs, keys=[p.split('/')[-1].split('.')[0] for p in file_paths])\n",
    "    combined_df.reset_index(level=0, inplace=True)\n",
    "    combined_df.rename(columns={'level_0': 'Ticker'}, inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "# Load stock data\n",
    "stock_file_paths = [\n",
    "    'C:/Users/hp/Desktop/week1/data/AAPL_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/AMZN_historical_data.csv',\n",
    "    'C:/Users/hp/Desktop/week1/data/GOOG_historical_data.csv'\n",
    "]\n",
    "stock_data = load_stock_data(stock_file_paths)\n",
    "\n",
    "# Load and process news data\n",
    "news_data = pd.read_csv(\"C:/Users/hp/Desktop/week1/data/raw_analyst_ratings.csv\")\n",
    "news_data.columns = news_data.columns.str.strip().str.lower()\n",
    "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
    "news_data.rename(columns={'date': 'Date', 'headline': 'Headline'}, inplace=True)\n",
    "\n",
    "# Standardize timezone\n",
    "def standardize_timezone(df):\n",
    "    if df['Date'].dt.tz is None:\n",
    "        df['Date'] = df['Date'].dt.tz_localize('UTC')\n",
    "    else:\n",
    "        df['Date'] = df['Date'].dt.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "news_data = standardize_timezone(news_data)\n",
    "stock_data = standardize_timezone(stock_data)\n",
    "\n",
    "# Merge data\n",
    "merged_data = pd.merge(news_data, stock_data, on='Date', how='outer')\n",
    "\n",
    "# Sentiment analysis and daily returns\n",
    "merged_data['Sentiment'] = merged_data['Headline'].apply(lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0.0)\n",
    "merged_data['Daily_Return'] = merged_data.groupby('Ticker')['Close'].pct_change() * 100\n",
    "\n",
    "# Drop rows with NaN values in 'Sentiment' or 'Daily_Return'\n",
    "clean_data = merged_data.dropna(subset=['Sentiment', 'Daily_Return'])\n",
    "\n",
    "# Check if there's enough data for correlation analysis\n",
    "if clean_data.empty or len(clean_data) < 2:\n",
    "    print(\"Not enough data for correlation analysis.\")\n",
    "else:\n",
    "    # Compute correlation\n",
    "    correlation = clean_data[['Sentiment', 'Daily_Return']].corr().iloc[0, 1]\n",
    "    print(f\"Correlation between news sentiment and daily stock returns: {correlation:.2f}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
